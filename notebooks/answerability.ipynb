{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - Answerability Intrinsic\n",
    "\n",
    "This notebook demonstrates some examples of using the Granite answerability intrinsic. It uses the shared IO processing code for intrinsics when performing model inference with an OpenAI-compatible backend such as vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go in this cell\n",
    "import openai\n",
    "import granite_common\n",
    "from granite_common import ChatCompletion\n",
    "from granite_common.base.types import UserMessage\n",
    "from granite_common.intrinsics.constants import BASE_MODEL_TO_CANONICAL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_name = \"answerability\"\n",
    "base_model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "\n",
    "# Change the following two constants as needed to reflect the location of the\n",
    "# inference server.\n",
    "openai_base_url = \"http://localhost:55555/v1\"\n",
    "openai_api_key = \"rag_intrinsics_1234\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate IO processing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch IO configuration file from Hugging Face Hub\n",
    "io_yaml_file = granite_common.intrinsics.util.obtain_io_yaml(\n",
    "    intrinsic_name, BASE_MODEL_TO_CANONICAL_NAME[base_model_name]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Instantiating input and output processing from configuration file:\\n\"\n",
    "    f\"{io_yaml_file}\"\n",
    ")\n",
    "\n",
    "intrinsics_rewriter = granite_common.IntrinsicsRewriter(config_file=io_yaml_file)\n",
    "intrinsincs_result_processor = granite_common.IntrinsicsResultProcessor(\n",
    "    config_file=io_yaml_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform input processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a user question and two documents.\n",
    "chat_input = ChatCompletion.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"assistant\", \"content\": \"Welcome to pet questions!\"},\n",
    "            {\"role\": \"user\", \"content\": \"Does my dog have fleas?\"},\n",
    "        ],\n",
    "        \"extra_body\": {\n",
    "            \"documents\": [\n",
    "                {\"doc_id\": \"1\", \"text\": \"My dog has fleas.\"},\n",
    "                {\"doc_id\": \"2\", \"text\": \"My cat does not have fleas.\"},\n",
    "            ],\n",
    "        },\n",
    "        \"model\": base_model_name,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    ")\n",
    "print(chat_input.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run request through input processing\n",
    "intrinsics_input = chat_input.model_copy(deep=True)\n",
    "intrinsics_input.model = intrinsic_name\n",
    "\n",
    "intrinsics_request = intrinsics_rewriter.transform(intrinsics_input)\n",
    "print(intrinsics_request.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the inference server\n",
    "client = openai.OpenAI(base_url=openai_base_url, api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our rewritten request directly to `chat.completions.create()`\n",
    "intrinsics_completion = client.chat.completions.create(\n",
    "    **intrinsics_request.model_dump()\n",
    ")\n",
    "\n",
    "print(intrinsics_request.messages[-1].content)\n",
    "print(intrinsics_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-process inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chat_completion = intrinsincs_result_processor.transform(\n",
    "    intrinsics_completion, intrinsics_request\n",
    ")\n",
    "\n",
    "print(\"After post-processing, first completion is:\")\n",
    "print(processed_chat_completion.choices[0].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some variations on the original question\n",
    "variations = [\n",
    "    \"Does my cat have no fleas?\",  # Answerable\n",
    "    \"Does my cat have green eyes?\",  # Unanswerable\n",
    "    \"Does my elephant have fleas?\",  # Unanswerable\n",
    "    \"Which of my pets have fleas?\",  # Answerable\n",
    "    \"What is the population of Australia?\",  # Unanswerable\n",
    "]\n",
    "\n",
    "for variation in variations:\n",
    "    updated_request = intrinsics_request.model_copy(deep=True)\n",
    "    updated_request.messages[-1] = UserMessage(content=variation)\n",
    "    chat_completion = client.chat.completions.create(**updated_request.model_dump())\n",
    "    print(f\"'{variation}' => {chat_completion.choices[0].message.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite-common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
