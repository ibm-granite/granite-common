{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to granite-common and the Granite RAG Intrinsics Library\n",
    "\n",
    "This notebook provides a high-level introduction to the `granite-common` library and to the [Granite RAG Intrinsics Library](https://huggingface.co/generative-computing/rag-intrinsics-lib).\n",
    "\n",
    "You will need a hosted vLLM server to perform inference. See the library above for scripts to host the models on your own server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other notebooks in this directory provide a more in-depth treatment of concepts covered\n",
    "in this notebook:\n",
    "\n",
    "* Intro to `granite-common` and simple interface to call each intrinsic: [intrinsics_openai.ipynb](./intrinsics_openai.ipynb) and [intrinsics_transformers.ipynb](./intrinsics_transformers.ipynb) \n",
    "* Advanced end-to-end Retrieval Augmented Generation flows: [rag.ipynb](./rag.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go in this cell\n",
    "import json\n",
    "import openai\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import granite_common\n",
    "from granite_common.base.types import (\n",
    "    ChatCompletion,\n",
    "    VLLMExtraBody,\n",
    ")\n",
    "from granite_common.retrievers.elasticsearch import ElasticsearchRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "CORPUS_NAMES_MAPPINGS = {\n",
    "    \"banking\": \"mt-rag-banking-elser-512-100-20250205\",\n",
    "    \"clapnq\": \"mt-rag-clapnq-elser-512-100-20240503\",\n",
    "    \"fiqa\": \"mt-rag-fiqa-beir-elser-512-100-20240501\",\n",
    "    \"govt\": \"mt-rag-govt-elser-512-100-20240611\",\n",
    "    \"ibmcloud\": \"mt-rag-ibmcloud-elser-512-100-20240502\",\n",
    "    \"scifact\": \"mt-rag-scifact-beir-elser-512-100-20240501\",\n",
    "    \"telco\": \"mt-rag-telco-elser-512-100-20241210\",\n",
    "}\n",
    "\n",
    "DEFAULT_CANNED_RESPONSE = (\n",
    "    \"Sorry, but I am unable to answer this question from the documents retrieved.\"\n",
    ")\n",
    "\n",
    "elasticsearch_host = \"https://localhost:32765\"\n",
    "elasticsearch_host = \"https://ibm_cloud_4ae4bca5_f6aa_43b6_93a3_befbd8fcb0e7:7d325be7af5de8c018b3284d754eb264995a56e4dfeba63fdeb6db1ff37dbd19@dbcc936c-8274-450e-9cb1-44a30ec26d88.c13paqsd05a0ept695ng.databases.appdomain.cloud:32765\"\n",
    "corpus_name = \"govt\"\n",
    "target_model_name = \"granite-3.3-8b-instruct\"\n",
    "base_model_name = f\"ibm-granite/{target_model_name}\"\n",
    "\n",
    "openai_base_url = \"http://localhost:55555/v1\"\n",
    "openai_base_url = \"http://p6-r19-n3.bluevela.rmf.ibm.com:55555/v1\"\n",
    "openai_api_key = \"rag_intrinsics_1234\"\n",
    "\n",
    "intrinsic_names = [\n",
    "    \"citations\",\n",
    "    \"query_rewrite\",\n",
    "    \"answerability\",\n",
    "    \"hallucination_detection\",\n",
    "    \"uncertainty\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsics\n",
    "# Load config files and create objects\n",
    "\n",
    "intrinsic_rewriters = {}\n",
    "intrinsic_result_processors = {}\n",
    "for intrinsic_name in intrinsic_names:\n",
    "    io_yaml_file = granite_common.intrinsics.util.obtain_io_yaml(\n",
    "        intrinsic_name, target_model_name\n",
    "    )\n",
    "\n",
    "    intrinsic_rewriter = granite_common.IntrinsicsRewriter(config_file=io_yaml_file)\n",
    "    intrinsic_result_processor = granite_common.IntrinsicsResultProcessor(\n",
    "        config_file=io_yaml_file\n",
    "    )\n",
    "\n",
    "    intrinsic_rewriters[intrinsic_name] = intrinsic_rewriter\n",
    "    intrinsic_result_processors[intrinsic_name] = intrinsic_result_processor\n",
    "\n",
    "# Connect to the inference server\n",
    "client = openai.OpenAI(base_url=openai_base_url, api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "# Connect to the Elasticsearch server.\n",
    "# Due to the setup, we have to open a retriever connection for each corpus.\n",
    "\n",
    "retrievers = {}\n",
    "for corpus_name, actual_corpus_name in CORPUS_NAMES_MAPPINGS.items():\n",
    "    retriever = ElasticsearchRetriever(\n",
    "        corpus_name=actual_corpus_name,\n",
    "        host=elasticsearch_host,\n",
    "        verify_certs=False,\n",
    "        ssl_show_warn=False,\n",
    "    )\n",
    "    retrievers[corpus_name] = retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "\n",
    "def call_intrinsic(\n",
    "    intrinsic_name: str,\n",
    "    chat_completion_request: dict,\n",
    "    **kwargs,\n",
    ") -> openai.types.chat.ChatCompletion:\n",
    "    \"\"\"\n",
    "    Call an intrinsic with OpenAI Python API objects on input and output.\n",
    "\n",
    "    :param intrinsic_name: Name of intrinsic to invoke\n",
    "    :param chat_completion_request: Chat completion request to make; can be dict or\n",
    "        OpenAI dataclass\n",
    "    :param kwargs: Optional named argument(s) for intrinsic\n",
    "\n",
    "    :returns: OpenAI Python API chat completion containing processed intrinsic outputs\n",
    "    \"\"\"\n",
    "    # Some intrinsics modify the chat object.\n",
    "    _chat_completion_request = chat_completion_request.model_copy(deep=True)\n",
    "\n",
    "    rewriter = intrinsic_rewriters[intrinsic_name]\n",
    "    result_processor = intrinsic_result_processors[intrinsic_name]\n",
    "    rewritten_request = rewriter.transform(_chat_completion_request, **kwargs)\n",
    "\n",
    "    # Set model name manually for now, because vLLM does not maintain any kind of\n",
    "    # metadata that would allow us to determine the right model name.\n",
    "    rewritten_request.model = intrinsic_name\n",
    "\n",
    "    response = client.chat.completions.create(**rewritten_request.model_dump())\n",
    "    # return response\n",
    "    transformed_response = result_processor.transform(response, rewritten_request)\n",
    "\n",
    "    # Convert to same type as OpenAI API\n",
    "    return openai.types.chat.ChatCompletion.model_validate(\n",
    "        transformed_response.model_dump()\n",
    "    )\n",
    "\n",
    "\n",
    "def retrieve_snippets(corpus_name: str, query: str, top_k: int = 3):\n",
    "    retriever = retrievers[corpus_name]\n",
    "    return retriever.retrieve(query, top_k=top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## granite-common\n",
    "\n",
    "TODO: Update description of `granite-common`\n",
    "\n",
    "The `granite-common` library provides input and output processing for large language models.\n",
    "In this context, *input and output processing* refers to the steps that happen \n",
    "immediately before and after low-level model inference. These steps include:\n",
    "\n",
    "* **Input processing:** Translating application data structures such as messages and \n",
    "  documents into a string prompt for a particular model\n",
    "* **Output processing:** Parsing the raw string output of a language model into \n",
    "  structured application data\n",
    "* **Constrained decoding:** Constraining the raw string output of an LLM to ensure that\n",
    "  the model's output will always parse into structured application data\n",
    "* **Inference-time scaling:** Extracting a higher-quality answer from an LLM by \n",
    "  combining the results of multiple inference calls.\n",
    "\n",
    "\n",
    "`granite-common` includes three main types of entry points:\n",
    "* **Backend connectors** connect the `granite-io` library to different model inference \n",
    "  engines and vector databases.\n",
    "  The other components of `granite-io` use these adapters to invoke model inference with\n",
    "  exactly the right low-level parameters for each model and inference layer.\n",
    "* **InputOutputProcessors** provide input and output processing for specific models.\n",
    "  An InputOutputProcessor exposes a \"chat completions\" interface, where the input is the\n",
    "  structured representation of a conversation and the output is the next turn of the\n",
    "  conversation.\n",
    "  For some models, such as [IBM Granite 3.3](https://huggingface.co/collections/ibm-granite/granite-33-language-models-67f65d0cca24bcbd1d3a08e3), we also provide\n",
    "  separate APIs that only perform input processing or output processing.\n",
    "* **RequestProcessors** rewrite chat completion requests in various ways, such as \n",
    "  rewording messages, attaching RAG documents, or filtering documents. You can chain\n",
    "  one or more RequestProcessors with an InputOutputProcessor to implement a custom \n",
    "  inference workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat completions API in `granite-common` runs low-level inference on the target\n",
    "model, passing in raw string prompts and inference paramters and receiving back raw \n",
    "string results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-2d105dce9c6641d686e06b7c59e4fcd0\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"17, 19, 23, \",\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null,\n",
      "      \"prompt_logprobs\": null,\n",
      "      \"prompt_token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1761939710,\n",
      "  \"model\": \"ibm-granite/granite-3.3-8b-instruct\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 12,\n",
      "    \"prompt_tokens\": 25,\n",
      "    \"total_tokens\": 37,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"service_tier\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = client.completions.create(\n",
    "    prompt=\"Complete this sequence: 2, 3, 5, 7, 11, 13, \",\n",
    "    model=base_model_name,\n",
    "    temperature=0.0,\n",
    "    max_tokens=12,\n",
    ")\n",
    "\n",
    "print(completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most users don't interact with the low-level backend API directly. The recommended way\n",
    "to use `granite-common` is via the InputOutputProcessor APIs, which convert high-level \n",
    "request into the specific combination of inference paramters that the model needs,\n",
    "run inference, and then convert the model's raw output into something that an \n",
    "application can use directly.\n",
    "\n",
    "Let's create an example chat completion request so we can show how the high-level \n",
    "InputOutputProcessor API works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Assistant:** Welcome to the City of Dublin, CA help desk.\n",
       "\n",
       "**User:** Hi there. Can you answer questions about fences?\n",
       "\n",
       "**Assistant:** Absolutely, I can provide general information about fences in Dublin, CA.\n",
       "\n",
       "**User:** Great. I want to add one in my front yard. Do I need a permit?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_input = ChatCompletion.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the City of Dublin, CA help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hi there. Can you answer questions about fences?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Absolutely, I can provide general information about \"\n",
    "                \"fences in Dublin, CA.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Great. I want to add one in my front yard. Do I need a \"\n",
    "                \"permit?\",\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def print_chat(c):\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"\\n\".join([f\"**{m.role.capitalize()}:** {m.content}\\n\" for m in c.messages])\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print_chat(chat_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chat completion request models a scenario where the user is talking to the \n",
    "automated help desk for the City of Dublin, CA and has just asked a question about \n",
    "permitting for installing fences. Running this chat completion request should produce\n",
    "an assistant response to this question.\n",
    "\n",
    "If we pass our chat completion (`chat_input`) to a `granite-common` InputOutputProcessor's \n",
    "`create_chat_completion()` method, the InputOutputProcessor will create a string prompt\n",
    "for the model, set up model-specific generation parameters, invoke model inference, and\n",
    "parse the model's raw output into a structured message.\n",
    "\n",
    "Here we create an InputOutputProcessor for the [IBM Granite 3.3](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-instruct) model and point that InputOutputProcessor at the backend we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, in Dublin, California, you typically need a permit for any new fence that exceeds 6 feet in height or is located within the front yard setback. The setback is the area between your property line and the street. It's usually 20 feet from the property line for front yard fences. However, these rules can vary based on specific circumstances, so it's always best to check with the Dublin Planning Department or visit their official website for the most accurate and current information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pass the example through Granite to get an answer.\n",
    "chat_input.model = base_model_name\n",
    "non_rag_completion = client.chat.completions.create(**chat_input.model_dump())\n",
    "\n",
    "display(Markdown(non_rag_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's response here is generic and vague, because the model's training data does \n",
    "not cover obscure zoning ordinances of small cities in northern California.\n",
    "\n",
    "We can use the \n",
    "[Uncertainty LoRA](\n",
    "    https://huggingface.co/generative-computing/core-intrinsics-lib/blob/main/uncertainty/README.md)\n",
    "model to flag cases such as this one that are not covered by the base model's \n",
    "training data. \n",
    "\n",
    "This model comes packaged as a LoRA adapter on top of Granite 3.3. To run the model, we\n",
    "create an instance of `CertaintyIOProcessor` -- the `granite-io` InputOutputProcessor\n",
    "for this model -- and point this InputOutputProcessor at a Backend that we have\n",
    "connected to the model's LoRA adapter. Then we can pass the same chat completion request\n",
    "into the model to compute a certainty score from 0 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainty score is 0.48 out of 1.0\n"
     ]
    }
   ],
   "source": [
    "response = call_intrinsic(\"uncertainty\", chat_input)\n",
    "certainty_score = round(json.loads(response.choices[0].message.content)[\"certainty\"], 2)\n",
    "\n",
    "print(f\"Certainty score is {certainty_score} out of 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low certainty score indicates that the model's training data does not align closely\n",
    "with this question.\n",
    "\n",
    "To answer this question properly, we need to provide the model with domain-specific \n",
    "information. One of the most popular ways to add domain-specific information to an LLM\n",
    "is to use the Retrieval-Augmented Generation (RAG) pattern. RAG involves retrieving\n",
    "snippets of text from a collection of documents and adding those snippets to the model's\n",
    "prompt.\n",
    "\n",
    "\n",
    "In this case, the relevant information can be found in the Government \n",
    "corpus of the [MTRAG multi-turn RAG benchmark](https://github.com/IBM/mt-rag-benchmark).\n",
    "Similar to its connectors for inference backends, `granite-io` has adapters for \n",
    "RAG retrieval backends.\n",
    "\n",
    "Let's spin up a connection in-memory vector database, using embeddings that we've \n",
    "precomputed offline from the MTRAG Government corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = InMemoryRetriever(embeddings_data_file, embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`granite-io` also includes a RequestProcessor that performs the retrieval phase of\n",
    "RAG. This class, called `RetrievalRequestProcessor`, takes as input a chat completion\n",
    "request. The RequestProcessor uses the text of the last user turn to query a `Retriever`\n",
    "instance and fetch document snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query is: 'Great. I want to add one in my front yard. Do I need a permit?'\n",
      "Matching document snippets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'doc_id': '2e07d61f7284722f-0-1459',\n",
       "  'text': \"SmartHome Security: municipality alarm permit | TELUS Support\\nYou're currently in Personal Support. If you're a Business customer, visit our\\n\\n[Business Support page.](https://www.telus.com/business/support)\\n\\n 1. [Support](/en/support)\\n 2. Alarm permit information by municipality\\n\\n# Alarm permit information by municipality\\n\\nSmartHome Security: Contact information for municipal alarm permits\\n\\n## Municipal alarm permits\\n\\nMost cities require an alarm permit for monitored systems. For details and to\\nsecure a permit, visit your city’s website.\\n\\nSome municipalities require you to have a permit in place by the time of your\\ninstallation. Please reach out to your local municipality to ensure you are\\nset up for success during the install.\\n\\nNote\\n\\n: LivingWell Companion devices and non-monitored security devices, such as\\nvideo cameras, do not require permits.\\n\\nPlease [contact us](https://www.telus.com/support/article/contacting-\\nsmarthome-support) to notify us of your permit number once you have it.\\n\\nProvince| Municipality| Webpage \\n---|---|--- \\nAlberta| Blackfalds|\\n[Blackfalds](https://blackfalds.ca/Home/DownloadDocument?docId=cbb58b49-ecd1-488c-a4c4-20e44b48ac56) \\n| Calgary| [Calgary](http://www.calgary.ca/cps/Pages/Public-services/Security-\\nand-alarm-permits-and-bylaws.aspx) \\n| Edmonton|\\n[Edmonton](https://www.edmontonpolice.ca/CommunityPolicing/OperationalServices/AlarmBylawProgram) \\n| Lethbridge| [Lethbridge](https://www.lethbridge.ca/City-\\nGovernment/Bylaws/Pages/default.aspx) \\n| Medicine Hat| [Medicine\",\n",
       "  'score': '5.9532413'},\n",
       " {'doc_id': 'ccc76432df3504b4-0-1459',\n",
       "  'text': \"SmartHome Security: municipality alarm permit | TELUS Support\\nYou're currently in Personal Support. If you're a Business customer, visit our\\n\\n[Business Support page.](https://www.telus.com/business/support)\\n\\n 1. [Support](/en/support)\\n 2. Alarm permit information by municipality\\n\\n# Alarm permit information by municipality\\n\\nSmartHome Security: Contact information for municipal alarm permits\\n\\n## Municipal alarm permits\\n\\nMost cities require an alarm permit for monitored systems. For details and to\\nsecure a permit, visit your city’s website.\\n\\nSome municipalities require you to have a permit in place by the time of your\\ninstallation. Please reach out to your local municipality to ensure you are\\nset up for success during the install.\\n\\nNote\\n\\n: LivingWell Companion devices and non-monitored security devices, such as\\nvideo cameras, do not require permits.\\n\\nPlease [contact us](https://www.telus.com/support/article/contacting-\\nsmarthome-support) to notify us of your permit number once you have it.\\n\\nProvince| Municipality| Webpage \\n---|---|--- \\nAlberta| Blackfalds|\\n[Blackfalds](https://blackfalds.ca/Home/DownloadDocument?docId=cbb58b49-ecd1-488c-a4c4-20e44b48ac56) \\n| Calgary| [Calgary](http://www.calgary.ca/cps/Pages/Public-services/Security-\\nand-alarm-permits-and-bylaws.aspx) \\n| Edmonton|\\n[Edmonton](https://www.edmontonpolice.ca/CommunityPolicing/OperationalServices/AlarmBylawProgram) \\n| Lethbridge| [Lethbridge](https://www.lethbridge.ca/City-\\nGovernment/Bylaws/Pages/default.aspx) \\n| Medicine Hat| [Medicine\",\n",
       "  'score': '5.9532413'},\n",
       " {'doc_id': '7aa4486df437286f-0-1459',\n",
       "  'text': \"SmartHome Security: municipality alarm permit | TELUS Support\\nYou're currently in Personal Support. If you're a Business customer, visit our\\n\\n[Business Support page.](https://www.telus.com/business/support)\\n\\n 1. [Support](/en/support)\\n 2. Alarm permit information by municipality\\n\\n# Alarm permit information by municipality\\n\\nSmartHome Security: Contact information for municipal alarm permits\\n\\n## Municipal alarm permits\\n\\nMost cities require an alarm permit for monitored systems. For details and to\\nsecure a permit, visit your city’s website.\\n\\nSome municipalities require you to have a permit in place by the time of your\\ninstallation. Please reach out to your local municipality to ensure you are\\nset up for success during the install.\\n\\nNote\\n\\n: LivingWell Companion devices and non-monitored security devices, such as\\nvideo cameras, do not require permits.\\n\\nPlease [contact us](https://www.telus.com/support/article/contacting-\\nsmarthome-support) to notify us of your permit number once you have it.\\n\\nProvince| Municipality| Webpage \\n---|---|--- \\nAlberta| Blackfalds|\\n[Blackfalds](https://blackfalds.ca/Home/DownloadDocument?docId=cbb58b49-ecd1-488c-a4c4-20e44b48ac56) \\n| Calgary| [Calgary](http://www.calgary.ca/cps/Pages/Public-services/Security-\\nand-alarm-permits-and-bylaws.aspx) \\n| Edmonton|\\n[Edmonton](https://www.edmontonpolice.ca/CommunityPolicing/OperationalServices/AlarmBylawProgram) \\n| Lethbridge| [Lethbridge](https://www.lethbridge.ca/City-\\nGovernment/Bylaws/Pages/default.aspx) \\n| Medicine Hat| [Medicine\",\n",
       "  'score': '5.9532413'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieval_request_proc = RetrievalRequestProcessor(retriever, top_k=3)\n",
    "# chat_input_with_docs = retrieval_request_proc.process(chat_input)[0]\n",
    "# chat_input_with_docs.model_dump()\n",
    "\n",
    "# The database fetches document snippets that match a given query.\n",
    "# For example, the user's question in the conversation above:\n",
    "query = chat_input.messages[-1].content\n",
    "print(f\"Query is: '{query}'\")\n",
    "print(\"Matching document snippets:\")\n",
    "documents = retrieve_snippets(corpus_name, query, top_k=3)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the last user turn in this conversation is:\n",
    "> **User:** Great. I want to add one in my front yard. Do I need a permit?\n",
    "\n",
    "This text is missing key details for retrieving relevant documents: What does the \n",
    "user want to add to their front yard, and what city's municipal code applies to this\n",
    "yard? As a result, the retrieved documents aren't actually relevant to the user's \n",
    "question.\n",
    "\n",
    "The [LoRA Adapter for Answerability Classification](\n",
    "    https://huggingface.co/generative-computing/rag-intrinsics-lib/blob/main/answerability/README.md)\n",
    "provides a robust way to detect this kind of problem. Here's what happens if we \n",
    "run the chat completion request with irrelevant document snippets through the \n",
    "answerability model, using the\n",
    "`granite_common` processor for the model to handle input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.146947860094409e-05"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieval step from before...\n",
    "chat_input_with_docs = chat_input.model_copy(deep=True)\n",
    "chat_input_with_docs.extra_body = VLLMExtraBody(documents=documents)\n",
    "chat_input_with_docs.model_dump()\n",
    "\n",
    "# ...followed by an answerability check\n",
    "response = call_intrinsic(\"answerability\", chat_input_with_docs)\n",
    "answerability_likelihood = json.loads(response.choices[0].message.content)[\n",
    "    \"answerability_likelihood\"\n",
    "]\n",
    "answerability_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use use the [LoRA Adapter for Query Rewrite](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib/blob/main/query_rewrite_lora/README.md) to rewrite\n",
    "the last user turn into a string that is more useful for retrieving document snippets.\n",
    "`granite-io` includes an InputOutputProcessor for running this model.\n",
    "Here's how to use this InputOutputProcessor to apply this model to our example \n",
    "conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do I need a permit to add a fence in my front yard in Dublin, CA?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = call_intrinsic(\"query_rewrite\", chat_input_with_docs)\n",
    "rewritten_question = json.loads(response.choices[0].message.content)[\n",
    "    \"rewritten_question\"\n",
    "]\n",
    "rewritten_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query rewrite model turns the last user turn in this conversation from:\n",
    "> **User:** Great. I want to add one in my front yard. Do I need a permit?\n",
    "\n",
    "...to a version of the same question that includes vital additional context:\n",
    "> **User:** Do I need a permit to add a fence in my front yard in Dublin, CA?\n",
    "\n",
    "This more specific query should allow the retriever to fetch better document snippets.\n",
    "\n",
    "The following code snippet uses `granite-io` APIs to rewrite the user query, then\n",
    "fetch relevant document snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'Welcome to the City of Dublin, CA help desk.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Hi there. Can you answer questions about fences?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Absolutely, I can provide general information about fences in Dublin, CA.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Great. I want to add one in my front yard. Do I need a permit?',\n",
       "   'role': 'user'}],\n",
       " 'model': 'ibm-granite/granite-3.3-8b-instruct',\n",
       " 'extra_body': {'documents': [{'text': \"When an underground cable line needs to be installed or replaced - Apoyo técnico de Xfinity\\n**Underground Utilities Locate Request Is Submitted (If Applicable):** Before our underground crew comes to your home, an order will be placed by the local [Dig Safe (811)](https://call811.com/) company to your local utilities (e.g., electric and gas companies) so they can mark where their utility lines are buried underground.\\n * The utility companies' locators use flags, spray paint or chalk that eventually washes away.\\n * This work must be completed before we can start digging, as it helps ensure that we don't damage any utility lines in the process.\\n * **Note:** You’re responsible for marking where any sprinkler system, electric dog fences, and/or private lighting, electric or gas lines are located to prevent them from being damaged. \\n 5. **Utilities Are Marked (If Applicable):** Once the utility companies' locators mark where their lines are, we're notified that it's safe for us to begin.\\n 6. **Underground Crew Assigned and Work Is Scheduled:** Once the utilities lines are marked, our underground crew comes out and installs the new underground cable line.\\n * If you requested to be home for the work or if you have pets or locked gates, we'll contact you to schedule the work.\\n * If you didn't request to be home, we'll notify you of the date range when the work should be completed.\\n 7. **Underground Cable Line Completed:** Your underground cable line has been installed or repaired.\\n * Si se había colocado una línea de cable temporal, esta se eliminará.\\n\\n### Retrasos\\n\\nLamentablemente, a veces tenemos retrasos. Los retrasos pueden ser causados\\npor:\\n\\n * **Permits:** In some communities, we're required to apply for and receive a permit before we can replace the underground cable line.\\n * Permit approvals can take **between five and 30 days**.\",\n",
       "    'doc_id': '9342877fdd60846d-2309-4078'},\n",
       "   {'text': 'SmartHome Security: municipality alarm permit | TELUS Support\\nalarm-permit-information%3FINTCMP%3DTcom-support_results-hub_municipal-alarm-\\npermits_municipality-alarm-permit-\\ninformation&r=&lt=1697&evt=pageLoad&sv=1&cdb=AQAQ&rn=456271)\\n\\n![](https://t.co/i/adsct?bci=3&dv=America%2FLos_Angeles%26en-\\nUS%2Cen%26Google%20Inc.%26MacIntel%26255%261792%261120%268%2624%261792%261013%260%26na&eci=2&event_id=bbdb6468-5e34-4430-ac83-9674da5b0b3e&events=%5B%5B%22pageview%22%2C%7B%7D%5D%5D&integration=advertiser&p_id=Twitter&p_user_id=0&pl_id=416187b6-4aa2-4b0d-a9e5-b46f750e1403&tw_document_href=https%3A%2F%2Fwww.telus.com%2Fen%2Fsupport%2Farticle%2Fmunicipality-\\nalarm-permit-information%3FINTCMP%3DTcom-support_results-hub_municipal-alarm-\\npermits_municipality-alarm-permit-\\ninformation&tw_iframe_status=0&tw_order_quantity=0&tw_sale_amount=0&txn_id=nxfde&type=javascript&version=2.3.31)![](https://analytics.twitter.com/i/adsct?bci=3&dv=America%2FLos_Angeles%26en-',\n",
       "    'doc_id': '7aa4486df437286f-6478-7377'},\n",
       "   {'text': 'SmartHome Security: municipality alarm permit | TELUS Support\\npermits_municipality-alarm-permit-\\ninformation&tw_iframe_status=0&tw_order_quantity=0&tw_sale_amount=0&txn_id=nxfde&type=javascript&version=2.3.31)![](https://analytics.twitter.com/i/adsct?bci=3&dv=America%2FLos_Angeles%26en-\\nUS%2Cen%26Google%20Inc.%26MacIntel%26255%261792%261120%268%2624%261792%261013%260%26na&eci=2&event_id=bbdb6468-5e34-4430-ac83-9674da5b0b3e&events=%5B%5B%22pageview%22%2C%7B%7D%5D%5D&integration=advertiser&p_id=Twitter&p_user_id=0&pl_id=416187b6-4aa2-4b0d-a9e5-b46f750e1403&tw_document_href=https%3A%2F%2Fwww.telus.com%2Fen%2Fsupport%2Farticle%2Fmunicipality-\\nalarm-permit-information%3FINTCMP%3DTcom-support_results-hub_municipal-alarm-\\npermits_municipality-alarm-permit-\\ninformation&tw_iframe_status=0&tw_order_quantity=0&tw_sale_amount=0&txn_id=nxfde&type=javascript&version=2.3.31)![dot\\nimage',\n",
       "    'doc_id': '7aa4486df437286f-7152-7973'}]},\n",
       " 'temperature': 0.0,\n",
       " 'max_tokens': 4096}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redo initialization so this cell can run independently of previous cells\n",
    "\n",
    "# Rewrite the last user turn into something more suitable for retrieval.\n",
    "response = call_intrinsic(\"query_rewrite\", chat_input)\n",
    "rewritten_question = json.loads(response.choices[0].message.content)[\n",
    "    \"rewritten_question\"\n",
    "]\n",
    "\n",
    "# Retrieve document snippets based on the rewritten turn and attach them to the chat\n",
    "# completion request.\n",
    "query = rewritten_question\n",
    "documents = retrieve_snippets(corpus_name, query, top_k=3)\n",
    "\n",
    "chat_input_with_docs = chat_input.model_copy(deep=True)\n",
    "chat_input_with_docs.extra_body = VLLMExtraBody(documents=documents)\n",
    "\n",
    "chat_input_with_docs.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attaching relevant information causes the model to respond with a more specific and \n",
    "detailed answer. Here's the result that we get when we pass the rewritten chat \n",
    "completion request to the InputOutputProcessor for Granite 3.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, according to the City of Dublin's regulations, a building permit is required for the construction of a new fence in the front yard. This is to ensure that the fence meets the city's setback requirements and other regulations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_completion = client.chat.completions.create(**chat_input_with_docs.model_dump())\n",
    "display(Markdown(rag_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer contains specific details about permits for building fences in Dublin, CA.\n",
    "These facts should grounded in documents retrieved from the corpus. We would like\n",
    "to be able to prove that the model used the data from the corpus and did not \n",
    "hallucinate a fictitious building code.\n",
    "\n",
    "We can use the [LoRA Adapter for Citation Generation](\n",
    "    https://huggingface.co/generative-computing/rag-intrinsics-lib/blob/main/citations/README.md\n",
    ") to explain exactly how this response is grounded in the documents that the rewritten\n",
    "user query retrieves. As with the other models we've shown so far, `granite-common` includes\n",
    "an InputOutputProcessor for this model. We can use this InputOutputProcessor to add\n",
    "citations to the assistant response from the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/dmfexp/8cc/kndtran/github/ibm-granite/granite-common/src/granite_common/base/types.py:42: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `UserMessage` - serialized value may not be as expected [field_name='messages', input_value=ChatCompletionMessage(con... reasoning_content=None), input_type=ChatCompletionMessage])\n",
      "  PydanticSerializationUnexpectedValue(Expected `AssistantMessage` - serialized value may not be as expected [field_name='messages', input_value=ChatCompletionMessage(con... reasoning_content=None), input_type=ChatCompletionMessage])\n",
      "  PydanticSerializationUnexpectedValue(Expected `ToolResultMessage` - serialized value may not be as expected [field_name='messages', input_value=ChatCompletionMessage(con... reasoning_content=None), input_type=ChatCompletionMessage])\n",
      "  PydanticSerializationUnexpectedValue(Expected `SystemMessage` - serialized value may not be as expected [field_name='messages', input_value=ChatCompletionMessage(con... reasoning_content=None), input_type=ChatCompletionMessage])\n",
      "  serialized_value = nxt(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, according to the City of Dublin's regulations, a building permit is required for the construction of a new fence in the front yard. This is to ensure that the fence meets the city's setback requirements and other regulations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citations:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Generate a Granite response.\n",
    "chat_input_citations = chat_input_with_docs.model_copy(deep=True)\n",
    "chat_input_citations.messages.append(rag_completion.choices[0].message)\n",
    "\n",
    "response = call_intrinsic(\"citations\", chat_input_citations)\n",
    "citations = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(chat_input_citations.messages[-1].content))\n",
    "print(\"Citations:\")\n",
    "print(json.dumps(citations, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update CitationsWidget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citations_io_proc = CitationsIOProcessor(citations_lora_backend)\n",
    "\n",
    "# # Add the assistant response to the original chat completion request\n",
    "# input_with_next_message = input.with_next_message(rag_result.results[0].next_message)\n",
    "\n",
    "# # Augment this response with citations to the RAG document snippets\n",
    "# results_with_citations = citations_io_proc.create_chat_completion(\n",
    "#     input_with_next_message\n",
    "# )\n",
    "# CitationsWidget().show(input_with_next_message, results_with_citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the [LoRA Adapter for Hallucination Detection in RAG outputs](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib/blob/main/hallucination_detection_lora/README.md\n",
    ") to check whether each sentence of the assistant response is consistent with the\n",
    "information in the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, according to the City of Dublin's regulations, a building permit is required for the construction of a new fence in the front yard. This is to ensure that the fence meets the city's setback requirements and other regulations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination Checks:\n",
      "[\n",
      "  {\n",
      "    \"response_begin\": 0,\n",
      "    \"response_end\": 142,\n",
      "    \"response_text\": \"Yes, according to the City of Dublin's regulations, a building permit is required for the construction of a new fence in the front yard. \",\n",
      "    \"faithfulness_likelihood\": 0.1011255451580745,\n",
      "    \"explanation\": \"This sentence makes a factual claim about the need for a building permit for a new fence in the front yard in Dublin, CA. However, the provided documents do not contain any information about the City of Dublin or its regulations regarding fences. Therefore, the faithfulness of this claim cannot be determined based on the provided context.\"\n",
      "  },\n",
      "  {\n",
      "    \"response_begin\": 142,\n",
      "    \"response_end\": 235,\n",
      "    \"response_text\": \"This is to ensure that the fence meets the city's setback requirements and other regulations.\",\n",
      "    \"faithfulness_likelihood\": 0.002521755487923711,\n",
      "    \"explanation\": \"This sentence provides a reason for the building permit requirement, but the provided documents do not contain any information about the City of Dublin or its regulations regarding fences. Therefore, the faithfulness of this claim cannot be determined based on the provided context.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Generate a Granite response.\n",
    "chat_input_hallucinations = chat_input_with_docs.model_copy(deep=True)\n",
    "chat_input_hallucinations.messages.append(rag_completion.choices[0].message)\n",
    "\n",
    "response = call_intrinsic(\"hallucination_detection\", chat_input_hallucinations)\n",
    "hallucinations = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(chat_input_hallucinations.messages[-1].content))\n",
    "print(\"Hallucination Checks:\")\n",
    "print(json.dumps(hallucinations, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `granite-io` library also allows developers to create their own custom \n",
    "InputOutputProcessors. For example, here's an InputOutputProcessor that rolls up the\n",
    "rewrite, retrieval, and citations processing steps from this notebook into a single\n",
    "`create_chat_completion()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from granite_common.base.types import (\n",
    "    ChatCompletion,\n",
    "    ChatCompletionResponse,\n",
    "    VLLMExtraBody,\n",
    ")\n",
    "from granite_common.retrievers.elasticsearch import ElasticsearchRetriever\n",
    "\n",
    "\n",
    "class MyRAGIOProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: openai.OpenAI,\n",
    "        retrievers: dict[str, ElasticsearchRetriever],\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.retrievers = retrievers\n",
    "\n",
    "    def call_intrinsic(\n",
    "        self,\n",
    "        intrinsic_name: str,\n",
    "        chat_completion_request: dict,\n",
    "        **kwargs,\n",
    "    ) -> openai.types.chat.ChatCompletion:\n",
    "        \"\"\"\n",
    "        Call an intrinsic with OpenAI Python API objects on input and output.\n",
    "\n",
    "        :param intrinsic_name: Name of intrinsic to invoke\n",
    "        :param chat_completion_request: Chat completion request to make;\n",
    "            can be dict or OpenAI dataclass\n",
    "        :param kwargs: Optional named argument(s) for intrinsic\n",
    "\n",
    "        :returns: OpenAI Python API chat completion containing processed\n",
    "            intrinsic outputs\n",
    "        \"\"\"\n",
    "        # Some intrinsics modify the chat object.\n",
    "        _chat_completion_request = chat_completion_request.model_copy(deep=True)\n",
    "\n",
    "        rewriter = intrinsic_rewriters[intrinsic_name]\n",
    "        result_processor = intrinsic_result_processors[intrinsic_name]\n",
    "        rewritten_request = rewriter.transform(_chat_completion_request, **kwargs)\n",
    "\n",
    "        # Set model name manually for now, because vLLM does not maintain any kind of\n",
    "        # metadata that would allow us to determine the right model name.\n",
    "        rewritten_request.model = intrinsic_name\n",
    "\n",
    "        response = self.client.chat.completions.create(**rewritten_request.model_dump())\n",
    "        # return response\n",
    "        transformed_response = result_processor.transform(response, rewritten_request)\n",
    "\n",
    "        # Convert to same type as OpenAI API\n",
    "        return openai.types.chat.ChatCompletion.model_validate(\n",
    "            transformed_response.model_dump()\n",
    "        )\n",
    "\n",
    "    def retrieve_snippets(self, corpus_name: str, query: str, top_k: int = 3) -> dict:\n",
    "        retriever = self.retrievers[corpus_name]\n",
    "        return retriever.retrieve(query, top_k=top_k)\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        chat_input: ChatCompletion,\n",
    "    ) -> ChatCompletionResponse:\n",
    "        \"\"\"Placeholder for a proper IO processor base class.\"\"\"\n",
    "\n",
    "        chat_input_with_docs = chat_input.model_copy(deep=True)\n",
    "\n",
    "        # Rewrite the last user turn for retrieval\n",
    "        response = self.call_intrinsic(\"query_rewrite\", chat_input_with_docs)\n",
    "        rewritten_question = json.loads(response.choices[0].message.content)[\n",
    "            \"rewritten_question\"\n",
    "        ]\n",
    "\n",
    "        # Retrieve documents with the rewritten last turn\n",
    "        query = rewritten_question\n",
    "        documents = self.retrieve_snippets(corpus_name, query, top_k=3)\n",
    "        chat_input_with_docs.extra_body = VLLMExtraBody(documents=documents)\n",
    "\n",
    "        # Generate a response\n",
    "        rag_completion = self.client.chat.completions.create(\n",
    "            **chat_input_with_docs.model_dump()\n",
    "        )\n",
    "        chat_response = chat_input_with_docs.model_copy(deep=True)\n",
    "        chat_response.messages.append(rag_completion.choices[0].message)\n",
    "\n",
    "        # Generate citations\n",
    "        chat_input_citations = chat_response.model_copy(deep=True)\n",
    "\n",
    "        response = self.call_intrinsic(\"citations\", chat_input_citations)\n",
    "        citations = json.loads(response.choices[0].message.content)\n",
    "\n",
    "        return chat_response, citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap all of the functionality we've shown so far in a single class that \n",
    "inherits from the `InputOutputProcessor` interface in `granite-io`. Packaging things\n",
    "this way lets applications treat this multi-step flow as if it was a single chat \n",
    "completion request to a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, according to the City of Dublin's regulations, a building permit is required for the construction of a new fence in the front yard. This is to ensure that the fence meets the city's setback requirements and other regulations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citations:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "chat_input = ChatCompletion.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the City of Dublin, CA help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hi there. Can you answer questions about fences?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Absolutely, I can provide general information about \"\n",
    "                \"fences in Dublin, CA.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Great. I want to add one in my front yard. Do I need a \"\n",
    "                \"permit?\",\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"model\": base_model_name,\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_io_proc = MyRAGIOProcessor(\n",
    "    client=client,\n",
    "    retrievers=retrievers,\n",
    ")\n",
    "\n",
    "rag_completion, citations = rag_io_proc.chat_completion(chat_input)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(rag_completion.messages[-1].content))\n",
    "print(\"Citations:\")\n",
    "print(json.dumps(citations, indent=2))\n",
    "# CitationsWidget().show(input_with_next_message, rag_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite-common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
