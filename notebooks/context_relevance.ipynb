{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - Context Relevance Intrinsic\n",
    "\n",
    "This notebook demonstrates some examples of using the [Granite context relevance intrinsic](https://huggingface.co/generative-computing/rag-intrinsics-lib/tree/main/context_relevance/README.md). It uses the shared IO processing code for intrinsics when performing model inference with an OpenAI-compatible backend such as vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go in this cell\n",
    "import openai\n",
    "import granite_common\n",
    "from granite_common import ChatCompletion\n",
    "from granite_common.intrinsics.constants import BASE_MODEL_TO_CANONICAL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_name = \"context_relevance\"\n",
    "base_model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "\n",
    "# Change the following two constants as needed to reflect the location of the\n",
    "# inference server.\n",
    "openai_base_url = \"http://localhost:55555/v1\"\n",
    "openai_api_key = \"rag_intrinsics_1234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I'm here to help you prepare for your job interview!\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I have a job interview next week for a marketing manager position.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"Congratulations! Marketing manager is an exciting role. \"\n",
    "            \"How are you feeling about it?\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"I'm nervous because I haven't interviewed in years, \"\n",
    "            \"and this is a big career move for me.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"It's natural to feel nervous, but preparation will help \"\n",
    "            \"boost your confidence.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"What should I expect them to ask about my experience with \"\n",
    "            \"social media campaigns as a marketing manager?\"\n",
    "        ),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate IO processing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch IO configuration file from Hugging Face Hub\n",
    "io_yaml_file = granite_common.intrinsics.util.obtain_io_yaml(\n",
    "    intrinsic_name, BASE_MODEL_TO_CANONICAL_NAME[base_model_name]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Instantiating input and output processing from configuration file:\\n\"\n",
    "    f\"{io_yaml_file}\"\n",
    ")\n",
    "\n",
    "intrinsics_rewriter = granite_common.IntrinsicsRewriter(config_file=io_yaml_file)\n",
    "intrinsincs_result_processor = granite_common.IntrinsicsResultProcessor(\n",
    "    config_file=io_yaml_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Document Context Relevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a user question and two documents.\n",
    "chat_input = ChatCompletion.model_validate(\n",
    "    {\n",
    "        \"messages\": base_messages,\n",
    "        \"extra_body\": {\n",
    "            \"documents\": [],\n",
    "        },\n",
    "        \"model\": base_model_name,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "intrinsics_kwargs = {\n",
    "    \"document_content\": \"Marketing manager interviews often focus on \\\n",
    "campaign experience and measurable results. \\\n",
    "Expect questions about social media ROI, audience engagement metrics, and \\\n",
    "conversion rates. \\\n",
    "Prepare specific examples of campaigns you've managed, including budget, \\\n",
    "timeline, and outcomes. \\\n",
    "Interviewers may ask about your experience with different social media \\\n",
    "platforms and their unique audiences. \\\n",
    "Be ready to discuss how you measure campaign success and adjust strategies \\\n",
    "based on performance data. \\\n",
    "Knowledge of current social media trends and emerging platforms demonstrates \\\n",
    "industry awareness.\",\n",
    "}\n",
    "\n",
    "print(chat_input.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run request through input processing\n",
    "intrinsics_input = chat_input.model_copy(deep=True)\n",
    "intrinsics_input.model = intrinsic_name\n",
    "\n",
    "intrinsics_request = intrinsics_rewriter.transform(\n",
    "    intrinsics_input, **intrinsics_kwargs\n",
    ")\n",
    "print(intrinsics_request.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the inference server\n",
    "client = openai.OpenAI(base_url=openai_base_url, api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our rewritten request directly to `chat.completions.create()`\n",
    "intrinsics_completion = client.chat.completions.create(\n",
    "    **intrinsics_request.model_dump()\n",
    ")\n",
    "\n",
    "print(intrinsics_request.messages[-1].content)\n",
    "print(intrinsics_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-process inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chat_completion = intrinsincs_result_processor.transform(\n",
    "    intrinsics_completion, intrinsics_request\n",
    ")\n",
    "\n",
    "print(\"After post-processing, first completion is:\")\n",
    "print(processed_chat_completion.choices[0].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partially Relevant Context Relevance Check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite-common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
