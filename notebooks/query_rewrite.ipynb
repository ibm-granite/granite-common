{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - Query Rewrite\n",
    "\n",
    "This notebook demonstrates some examples of using the Granite query rewrite intrinsic. It uses the shared IO processing code for intrinsics when performing model inference with an OpenAI-compatible backend such as vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go in this cell\n",
    "import openai\n",
    "import granite_common\n",
    "from granite_common import ChatCompletion\n",
    "from granite_common.intrinsics.constants import BASE_MODEL_TO_CANONICAL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_name = \"query_rewrite\"\n",
    "base_model_name = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "\n",
    "# Change the following two constants as needed to reflect the location of the\n",
    "# inference server.\n",
    "openai_base_url = \"http://localhost:55555/v1\"\n",
    "openai_api_key = \"rag_intrinsics_1234\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate IO processing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch IO configuration file from Hugging Face Hub\n",
    "io_yaml_file = granite_common.intrinsics.util.obtain_io_yaml(\n",
    "    intrinsic_name, BASE_MODEL_TO_CANONICAL_NAME[base_model_name]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Instantiating input and output processing from configuration file:\\n\"\n",
    "    f\"{io_yaml_file}\"\n",
    ")\n",
    "\n",
    "intrinsics_rewriter = granite_common.IntrinsicsRewriter(config_file=io_yaml_file)\n",
    "intrinsincs_result_processor = granite_common.IntrinsicsResultProcessor(\n",
    "    config_file=io_yaml_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform input processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a user question and two documents.\n",
    "chat_input = ChatCompletion.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"assistant\", \"content\": \"Welcome to pet questions!\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I have two pets, a dog named Rex and a cat named Lucy.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Great, what would you like to share about them?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Rex spends a lot of time in the backyard and outdoors, \"\n",
    "                \"and Luna is always inside.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Sounds good! Rex must love exploring outside, while Lucy \"\n",
    "                \"probably enjoys her cozy indoor life.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"But is he more likely to get fleas because of that?\",\n",
    "            },\n",
    "        ],\n",
    "        \"model\": base_model_name,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    ")\n",
    "print(chat_input.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run request through input processing\n",
    "intrinsics_input = chat_input.model_copy(deep=True)\n",
    "intrinsics_input.model = intrinsic_name\n",
    "\n",
    "intrinsics_request = intrinsics_rewriter.transform(intrinsics_input)\n",
    "print(intrinsics_request.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the inference server\n",
    "client = openai.OpenAI(base_url=openai_base_url, api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our rewritten request directly to `chat.completions.create()`\n",
    "intrinsics_completion = client.chat.completions.create(\n",
    "    **intrinsics_request.model_dump()\n",
    ")\n",
    "\n",
    "print(intrinsics_request.messages[-1].content)\n",
    "print(intrinsics_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-process inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chat_completion = intrinsincs_result_processor.transform(\n",
    "    intrinsics_completion, intrinsics_request\n",
    ")\n",
    "\n",
    "print(processed_chat_completion.choices[0].model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite-common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
